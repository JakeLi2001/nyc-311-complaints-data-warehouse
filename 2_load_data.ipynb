{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fac4b87-b835-4472-8df6-7c5444846d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import pandas as pd\n",
    "import os\n",
    "# import pyarrow\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b210ac-7c07-47ed-8520-a7219b4c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_name = 'agency'\n",
    "\n",
    "surrogate_key = f\"{dimension_name}_dim_id\"\n",
    "\n",
    "business_key = f'{dimension_name}_id'\n",
    "\n",
    "gcp_project = 'cis4400-381214'\n",
    "bq_dataset = '311_complaints_dataset'\n",
    "table_name = f\"{dimension_name}_dimension\"\n",
    "dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
    "\n",
    "file_source_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0990204-8c09-4728-8918-e00148cbc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "current_date = datetime.today().strftime('%Y%m%d')\n",
    "log_filename = \"_\".join([\"etl\",dimension_name,current_date])+\".log\"\n",
    "logging.basicConfig(filename=log_filename, encoding='utf-8', format='%(asctime)s %(message)s', level=logging.DEBUG)\n",
    "logging.info(\"=========================================================================\")\n",
    "logging.info(f\"Starting ETL Run for dimension {dimension_name} on date {current_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dfe1f91-cdc3-4f9b-b3a8-6845878a8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data_file(logging: logging.Logger,\n",
    "                      file_source_path: str,\n",
    "                      file_name: str,\n",
    "                      df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    load_csv_data_file\n",
    "    Accepts a file source path and a file name\n",
    "    Loads the file into a data frame\n",
    "    Exits the program on error\n",
    "    Returns the dataframe\n",
    "    \"\"\"\n",
    "    file_source = os.path.join(file_source_path, file_name)\n",
    "    logging.info(f\"Reading source data file: {file_source}\")\n",
    "    # Read in the source data file for the customers data\n",
    "    try:\n",
    "        df = pd.read_csv(file_source)\n",
    "        # Set all of the column names to lower case letters\n",
    "        df = df.rename(columns=str.lower)\n",
    "        logging.info(f\"Read {len(df)} records from source data file: {file_source}\")\n",
    "        return df\n",
    "    except:\n",
    "        logging.error(f\"Failed to read file: {file_source}\")\n",
    "        # os._exit(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd5f676-3372-4054-9cc7-9869774e5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(logging: logging.Logger,\n",
    "                   df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    transform_data\n",
    "    Accepts a data frame\n",
    "    Performs any specific cleaning and transformation steps on the dataframe\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    # Convert the date_of_birth to a datetime64 data type. 2012-08-21 04:12:16.827\n",
    "    logging.info(\"Transforming dataframe.\")\n",
    "    # Select the columns for this dimension\n",
    "    column_list = ['agency', 'agency_name']\n",
    "    df = df[column_list]\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945a2a03-36fe-4130-9e2c-4672d2214bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_client(logging):\n",
    "    \"\"\"\n",
    "    create_bigquery_client\n",
    "    Creates a BigQuery client using the path to the service account key file\n",
    "    for credentials.\n",
    "    Returns the BigQuery client object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bqclient = bigquery.Client.from_service_account_json('keys/new-cis4400-381214-f4f2229d6853.json')\n",
    "        # Google Colab authentication already completed\n",
    "        # bqclient = bigquery.Client(gcp_project)\n",
    "        logging.info(\"Created BigQuery Client: %s\", bqclient)\n",
    "        return bqclient\n",
    "    except Exception as err:\n",
    "        logging.error(\"Failed to create BigQuery Client.\", err)\n",
    "        # os._exit(-1)\n",
    "    return bqclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0742b3fc-34af-4c42-a482-9203242ea021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bigquery_table(logging, bqclient, table_path, write_disposition, df):\n",
    "    \"\"\"\n",
    "    upload_bigquery_table\n",
    "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
    "    Loads the data into the BigQuery table from the dataframe.\n",
    "    for credentials.\n",
    "    The write disposition is either\n",
    "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.   \n",
    "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Creating BigQuery Job configuration with write_disposition=%s\", write_disposition)\n",
    "        # Set up a BigQuery job configuration with the write_disposition.\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "        # Submit the job\n",
    "        logging.info(\"Submitting the BigQuery job\")\n",
    "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)  \n",
    "        # Show the job results\n",
    "        logging.info(\"Job  results: %s\",job.result())\n",
    "    except Exception as err:\n",
    "        logging.error(\"Failed to load BigQuery Table. %s\", err)\n",
    "        #os._exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192573a0-1a96-41a6-a8db-0b455c65a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_table_exists(bqclient, table_path):\n",
    "    \"\"\"\n",
    "    bigquery_table_exists\n",
    "    Accepts a path to a BigQuery table\n",
    "    Checks if the BigQuery table exists.\n",
    "    Returns True or False\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        bqclient.get_table(table_path)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a282c2-66af-4fb1-b8ed-77417d03557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_table(logging, table_path, bqclient, surrogate_key):\n",
    "    \"\"\"\n",
    "    query_bigquery_table\n",
    "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
    "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
    "    Returns the dataframe\n",
    "    \"\"\"    \n",
    "    bq_df = pd.DataFrame\n",
    "    sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
    "    logging.info(\"Running query: %s\", sql_query)\n",
    "    try:\n",
    "        bq_df = bqclient.query(sql_query).to_dataframe()\n",
    "    except Exception as err:\n",
    "        logging.info(\"Error querying the table. %s\", err)\n",
    "    return bq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed348482-3b38-4062-8c94-1dbac92f235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surrogate_key(df, dimension_name='customers', offset=1):\n",
    "    \"\"\"\n",
    "    add_surrogate_key  \n",
    "    Accepts a data frame and inserts an integer identifier as the first column\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    # Reset the index to count from 0\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # Add the new surrogate key starting from offset\n",
    "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6caf26a-62b7-451a-bd73-286739c471c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_update_date(df, current_date):\n",
    "    \"\"\"\n",
    "    add_update_date\n",
    "    Accepts a data frame and inserts the current date as a new field\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    df['update_date'] = pd.to_datetime(current_date)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85021ca6-58a2-42d5-8548-e2b1b5d3f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_update_timestamp(df):\n",
    "    \"\"\"\n",
    "    add_update_timestamp\n",
    "    Accepts a data frame and inserts the current datetime as a new field\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    df['update_timestamp'] = pd.to_datetime('now', utc=True).replace(microsecond=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91aae451-4b88-4260-beb7-93707fa439ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_table(logging, bqclient, dimension_table_path, dimension_name, df):\n",
    "    \"\"\"\n",
    "    build_new_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame \n",
    "    Add the surrogate key and a record timestamp to the data frame\n",
    "    Inserts the contents of the dataframe to the dimensional table.\n",
    "    \"\"\"\n",
    "    logging.info(\"Target dimension table %s does not exit\", dimension_table_path)\n",
    "    # Add a surrogate key\n",
    "    df = add_surrogate_key(df, dimension_name, 1)\n",
    "    # Add the update timestamp\n",
    "    df = add_update_timestamp(df)\n",
    "    # Upload the dataframe to the BigQuery table\n",
    "    upload_bigquery_table(logging, bqclient, dimension_table_path, \"WRITE_TRUNCATE\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786f24ad-7bd9-4c59-8941-85375894252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_existing_table(logging, bqclient, dimension_table_path, dimension_name, surrogate_key, df):\n",
    "    \"\"\"\n",
    "    insert_existing_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame \n",
    "    Compares the new data to the existing data in the table.\n",
    "    Inserts the new/modified records to the existing table\n",
    "    \"\"\"\n",
    "    bq_df = pd.DataFrame\n",
    "    logging.info(\"Target dimension table %s exits. Checking for differences.\", dimension_table_path)\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)\n",
    "    # Compare with the new data set\n",
    "    new_records_df = pd.concat([df,bq_df]).drop_duplicates(keep=False)\n",
    "    logging.info(\"Found %d new records.\", new_records_df.shape[0])\n",
    "    if new_records_df.shape[0] > 0:\n",
    "        # Set the surrogate key for the new records. bq_df.shape[0] is number of records already in the database\n",
    "        new_surrogate_key_value = bq_df.shape[0]+1\n",
    "        new_records_df = add_surrogate_key(new_records_df, dimension_name, new_surrogate_key_value)\n",
    "        # Add the current date for the new records\n",
    "        new_records_df = add_update_timestamp(new_records_df)\n",
    "        # Upload the new records into the dimension table\n",
    "        upload_bigquery_table(logging, bqclient, dimension_table_path, \"WRITE_APPEND\", new_records_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b4d2cd-9725-44a9-88d7-b60422d891c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_969/2777159768.py:16: DtypeWarning: Columns (26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_source)\n"
     ]
    }
   ],
   "source": [
    "# Program main\n",
    "# Load the CSV File into a dataframe\n",
    "# Transform the Dataframe\n",
    "# Create a BigQuery client\n",
    "# See if the target dimension table exists\n",
    "#    If not exists, load the data into a new table\n",
    "#    If exists, insert new records into the table\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame\n",
    "    # Load in the data file\n",
    "    df = load_csv_data_file(logging, file_source_path, \"311_traffic_signal_complaints_2019.csv\", df)\n",
    "    # Transform the data\n",
    "    df = transform_data(logging, df)\n",
    "    # Create the BigQuery Client\n",
    "    bqclient = create_bigquery_client(logging)\n",
    "    # See if the target dimensional table exists\n",
    "    target_table_exists = bigquery_table_exists(bqclient, dimension_table_path)\n",
    "    # If the target dimension table does not exist, load all of the data into a new table\n",
    "    if not target_table_exists:\n",
    "        build_new_table(logging, bqclient, dimension_table_path, dimension_name, df)\n",
    "    # If the target table exists, then perform an incremental load    \n",
    "    if target_table_exists:\n",
    "        insert_existing_table(logging, bqclient, dimension_table_path, dimension_name, surrogate_key, df)\n",
    "    # Flush the log file\n",
    "    logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8b404-b31c-44bd-b651-1cf3a6c226f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991d9b8-2603-48b2-86ce-55f30542a792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d3802-0f08-4077-9e87-5ed165ba047c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe7faf-bf8b-42a2-98de-ae4728b0fb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b83ba-caeb-4a7f-aeba-2a1d2cac1e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyc-311-dw]",
   "language": "python",
   "name": "conda-env-nyc-311-dw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

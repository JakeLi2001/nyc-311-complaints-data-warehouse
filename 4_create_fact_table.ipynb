{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28866e7f-d317-467c-88e7-0d3cbd97f5f1",
   "metadata": {},
   "source": [
    "# Import libraries and functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b21be1a-3937-4a1b-b99e-53a7723f1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3febe5a-e7b7-42bb-b3cd-97d75c9c430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame\n",
    "\n",
    "fact_name = 'complaints'\n",
    "\n",
    "gcp_project = 'cis4400-381214' # replace to your own project id\n",
    "bq_dataset = '311_complaints_dataset' # replace to your own dataset name\n",
    "table_name = fact_name + '_fact'\n",
    "\n",
    "fact_table_path = '.'.join([gcp_project,bq_dataset,table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aca1190-74f6-4e40-86db-9946a2a778fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "current_date = datetime.today().strftime('%Y%m%d')\n",
    "log_filename = '_'.join(['etl_complaint_fact_',current_date])+'.log'\n",
    "logging.basicConfig(filename=log_filename, encoding='utf-8', format='%(asctime)s %(message)s', level=logging.DEBUG)\n",
    "logging.info('=========================================================================')\n",
    "logging.info('Starting ETL Run for complaint fact on date '+current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448f7d79-e487-47d1-9b00-66dca2e4f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data_file(logging, file_name, df):\n",
    "    '''\n",
    "    load_csv_data_file\n",
    "    Accepts a file source path and a file name\n",
    "    Loads the file into a data frame\n",
    "    Exits the program on error\n",
    "    Returns the dataframe\n",
    "    '''\n",
    "    logging.info(f'Reading source data file: {file_name}')\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "        df = df.rename(columns=str.lower)\n",
    "        logging.info(f'Read {len(df)} records from source data file: {file_name}')\n",
    "        return df\n",
    "    except:\n",
    "        logging.error(f'Failed to read file: {file_name}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3076ac4a-ded8-47b8-9f5d-df81ce53c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_client(logging):\n",
    "    '''\n",
    "    create_bigquery_client\n",
    "    Creates a BigQuery client using the path to the service account key file\n",
    "    for credentials.\n",
    "    Returns the BigQuery client object\n",
    "    '''\n",
    "    try:\n",
    "        bqclient = bigquery.Client.from_service_account_json('keys/new-cis4400-381214-f4f2229d6853.json') # replace with your own SA keys\n",
    "        logging.info('Created BigQuery Client: %s',bqclient)\n",
    "        return bqclient\n",
    "    except Exception as err:\n",
    "        logging.error('Failed to create BigQuery Client.', err)\n",
    "        # os._exit(-1)\n",
    "    return bqclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51abadcc-c5c8-4fb8-b789-ea4a5105c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bigquery_table(logging, bqclient, table_path, write_disposition, df):\n",
    "    '''\n",
    "    upload_bigquery_table\n",
    "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
    "    Loads the data into the BigQuery table from the dataframe.\n",
    "    for credentials.\n",
    "    The write disposition is either\n",
    "    write_disposition='WRITE_TRUNCATE'  Erase the target data and load all new data.   \n",
    "    write_disposition='WRITE_APPEND'    Append to the existing table\n",
    "    '''\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "        # Submit the job\n",
    "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)  \n",
    "        # Show the job results\n",
    "        job.result()\n",
    "    except Exception as err:\n",
    "        logging.error('Failed to load BigQuery Table.', err)\n",
    "        # os._exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ddc3a5-0add-43de-a12b-16db1fd8f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_table_exists(table_path, bqclient):\n",
    "    '''\n",
    "    bigquery_table_exists\n",
    "    Accepts a path to a BigQuery table\n",
    "    Checks if the BigQuery table exists.\n",
    "    Returns True or False\n",
    "    '''    \n",
    "    try:\n",
    "        bqclient.get_table(table_path)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        # print('Table {} is not found.'.format(table_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0f0107e-bd02-47de-aaf2-328dd3008e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_table(logging, table_path, bqclient, surrogate_key):\n",
    "    '''\n",
    "    query_bigquery_table\n",
    "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
    "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
    "    Returns the dataframe\n",
    "    '''    \n",
    "    bq_df = pd.DataFrame\n",
    "    # sql_query = 'SELECT * EXCEPT ( update_timestamp, ' + surrogate_key + ') FROM `' + table_path + '`'\n",
    "    sql_query = 'SELECT * EXCEPT (update_timestamp) FROM `' + table_path + '`'\n",
    "    logging.info('Running query: %s', sql_query)\n",
    "    bq_df = bqclient.query(sql_query).to_dataframe()\n",
    "    return bq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21006bd-184c-4c4b-8be1-443e8b1ab204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surrogate_key(df, dimension_name, offset=1):\n",
    "    '''\n",
    "    add_surrogate_key  \n",
    "    Accepts a data frame and inserts an integer identifier as the first column\n",
    "    Returns the modified dataframe\n",
    "    '''\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # Add the new surrogate key starting from offset\n",
    "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69ba207-699f-48ec-8175-d14dc99f5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_update_date(df, current_date):\n",
    "    '''\n",
    "    add_update_date\n",
    "    Accepts a data frame and inserts the current date as a new field\n",
    "    Returns the modified dataframe\n",
    "    '''\n",
    "    df['update_date'] = pd.to_datetime(current_date)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7186639a-0a54-4857-9c58-3cebfb8eeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_update_timestamp(df):\n",
    "    '''\n",
    "    add_update_timestamp\n",
    "    Accepts a data frame and inserts the current datetime as a new field\n",
    "    Returns the modified dataframe\n",
    "    '''\n",
    "    df['update_timestamp'] = pd.Timestamp('now', tz='utc').replace(microsecond=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c57bed-486f-4fa1-8a98-ac7166ec07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_table(logging, bqclient, table_path, df):\n",
    "    '''\n",
    "    build_new_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame \n",
    "    Add the surrogate key and a record timestamp to the data frame\n",
    "    Inserts the contents of the dataframe to the dimensional table.\n",
    "    '''\n",
    "    logging.info('Target table %s does not exit', table_path)\n",
    "    upload_bigquery_table(logging, bqclient, table_path, 'WRITE_TRUNCATE', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2638357e-02b1-403a-aaf3-7fcd54c1b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_existing_table(logging, bqclient, table_path, df):\n",
    "    '''\n",
    "    insert_existing_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame \n",
    "    Compares the new data to the existing data in the table.\n",
    "    Inserts the new/modified records to the existing table\n",
    "    '''\n",
    "    logging.info('Target table %s exits. Appending records.', table_path)\n",
    "    upload_bigquery_table(logging, bqclient, table_path, 'WRITE_APPEND', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4c2d2b-9a2a-4d22-bde4-b7f0c604d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_lookup(logging, dimension_name, lookup_columns, df=df):\n",
    "    '''\n",
    "    dimension_lookup\n",
    "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
    "    Returns dataframe augmented with the surrogate keys\n",
    "    '''\n",
    "    bq_df = pd.DataFrame\n",
    "    logging.info('Lookup dimension %s.', dimension_name)\n",
    "    surrogate_key = dimension_name + '_dim_id'\n",
    "    dimension_table_path = '.'.join([gcp_project, bq_dataset, dimension_name + '_dimension'])\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)\n",
    "    # print(bq_df)\n",
    "    # Melt the dimension dataframe into an index with the lookup columns\n",
    "    m = bq_df.melt(id_vars=lookup_columns, value_vars=surrogate_key)\n",
    "    # print(m)\n",
    "    # Rename the 'value' column to the surrogate key column name\n",
    "    m = m.rename(columns={'value':surrogate_key})\n",
    "    # Merge with the fact table record\n",
    "    df = df.merge(m, on=lookup_columns, how='left')\n",
    "    # Drop the 'variable' column and the lookup columns\n",
    "    df = df.drop(columns=lookup_columns)\n",
    "    df = df.drop(columns='variable')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dd816a9-9950-4fde-bc94-827ac7833766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_dimension_lookup(logging, dimension_name, lookup_column, df=df):\n",
    "    '''\n",
    "    dimension_lookup\n",
    "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
    "    Returns dataframe augmented with the surrogate keys\n",
    "    '''\n",
    "    bq_df = pd.DataFrame\n",
    "    logging.info('Lookup date dimension on column %s.', lookup_column)\n",
    "    surrogate_key = dimension_name + '_dim_id'\n",
    "    dimension_table_path = '.'.join([gcp_project, bq_dataset, dimension_name + '_dimension'])\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)\n",
    "    bq_df['full_date'] = pd.to_datetime(bq_df['full_date'])\n",
    "    # Return just the date portion\n",
    "    bq_df['full_date'] = bq_df.full_date.dt.date\n",
    "    \n",
    "    # Extract the date from 'created_date' column\n",
    "    df[lookup_column] = pd.to_datetime(df[lookup_column])\n",
    "    # Strip off the time portion\n",
    "    df[lookup_column + '_time'] = df[lookup_column].dt.strftime('%H:%M:%S')\n",
    "    # Return just the date portion\n",
    "    df[lookup_column] = df[lookup_column].dt.date\n",
    "    \n",
    "    # Melt the dimension dataframe into an index with the lookup columns\n",
    "    m = bq_df.melt(id_vars='full_date', value_vars=surrogate_key)\n",
    "    # Rename the 'value' column to the surrogate key column name\n",
    "    m = m.rename(columns={'value':lookup_column+'_dim_id'})\n",
    "    \n",
    "    # Merge with the fact table record on the created_date\n",
    "    df = df.merge(m, left_on=lookup_column, right_on='full_date', how='left')\n",
    "\n",
    "    # Drop the 'variable' column and the lookup columns\n",
    "    df = df.drop(columns=lookup_column)\n",
    "    df = df.drop(columns='variable')\n",
    "    df = df.drop(columns='full_date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c60da8-1a94-445e-bf5e-67a3e8dcebcd",
   "metadata": {},
   "source": [
    "# Create fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d9740f4-f480-4b0b-97e8-424131be9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2019, 2024):\n",
    "    logging.info('=========================================================================')\n",
    "    if __name__ == '__main__':\n",
    "        df = pd.DataFrame\n",
    "        # Create the BigQuery Client\n",
    "        bqclient = create_bigquery_client(logging)\n",
    "        # Load in the data file\n",
    "        df = load_csv_data_file(logging, f'data/311_traffic_signal_complaints_{year}.csv', df)\n",
    "        # If city is empty, fill it in with NEW YORK\n",
    "        df.city = df.city.fillna('NEW YORK')\n",
    "        # Consider removing columns that we will never use  df.drop([....])\n",
    "\n",
    "        # Lookup the agency dimension record  agency_dim_id\n",
    "        df = dimension_lookup(logging, dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df)\n",
    "\n",
    "        # Lookup the location dimension record  location_dim_id\n",
    "        df = dimension_lookup(logging, dimension_name='location', lookup_columns=['incident_zip', 'intersection_street_1', 'intersection_street_2', 'borough'], df=df)\n",
    "\n",
    "        # Lookup the channel  dimension record  channel_dim_id\n",
    "        df = dimension_lookup(logging, dimension_name='complaints_status', lookup_columns=['status'], df=df)\n",
    "\n",
    "        # Lookup the complaint_type  dimension record  complaint_type_dim_id\n",
    "        df = dimension_lookup(logging, dimension_name='complaint_type', lookup_columns=['complaint_type', 'descriptor'], df=df)\n",
    "\n",
    "\n",
    "        # Lookup the created_date dimension record  \n",
    "        df = date_dimension_lookup(logging, dimension_name='date', lookup_column='created_date', df=df)\n",
    "        # Lookup the closed_date dimension record\n",
    "        df = date_dimension_lookup(logging, dimension_name='date', lookup_column='closed_date', df=df)\n",
    "\n",
    "        # A list of all of the surrogate keys\n",
    "        # For transaction grain, also include the 'unique_key' column\n",
    "        surrogate_keys=['agency_dim_id','location_dim_id','complaints_status_dim_id','complaint_type_dim_id','created_date_dim_id','closed_date_dim_id']\n",
    "\n",
    "        # Remove all of the other non-surrogate key columns\n",
    "        df = df[surrogate_keys]\n",
    "\n",
    "        # For daily snapshot grain we:\n",
    "        # 1) Add a 'complaint_count' fact\n",
    "        # 2) Use Group By to count up the number of complaints, per location, per agency, etc. per day\n",
    "        # For transaction grain add in the unique_key but skip the above two steps.\n",
    "\n",
    "        # Add a complaint count (for daily snapshot grain)\n",
    "        df['complaint_count'] = 1\n",
    "        # Count up the number of complaints per agency, per location, etc. per day\n",
    "        df = df.groupby(surrogate_keys)['complaint_count'].agg('count').reset_index()\n",
    "\n",
    "        # See if the target table exists\n",
    "        target_table_exists = bigquery_table_exists(fact_table_path, bqclient)\n",
    "        # If the target table does not exist, load all of the data into a new table\n",
    "        if not target_table_exists:\n",
    "            build_new_table(logging, bqclient, fact_table_path, df)\n",
    "        # If the target table exists, then perform an incremental load    \n",
    "        if target_table_exists:\n",
    "            insert_existing_table(logging, bqclient, fact_table_path, df)\n",
    "        logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c586382-4d6d-453c-8f5a-aa17fe4cd4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fc527-20a6-4bfc-aa13-61c9fff4af7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cee0e7-e8cd-4238-a527-42ed2070d6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyc-311-dw]",
   "language": "python",
   "name": "conda-env-nyc-311-dw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
